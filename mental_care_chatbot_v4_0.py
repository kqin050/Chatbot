# -*- coding: utf-8 -*-
"""Mental Care Chatbot_v4_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AcDCG1O6X1e8qR3PnDRnsQMB_KmIMV1h
"""

import nltk
import numpy as np
import random
import pandas as pd
import re
from collections import Counter
from string import punctuation
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import datasets
from sklearn import preprocessing
from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score
from scipy.stats import mode
#Text Featuring
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.decomposition import LatentDirichletAllocation#LDA
#svm
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics, neighbors
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from matplotlib.colors import ListedColormap
from sklearn.metrics import classification_report,precision_recall_curve
#confusion martrix
from sklearn.metrics import confusion_matrix
import seaborn as sns
#KNN
from sklearn.neighbors import KNeighborsClassifier
from sklearn import model_selection
#Cross vaildation
from sklearn import tree
nltk.download('stopwords')
nltk.download('punkt')
stop_words=set(stopwords.words("english"))
print(stop_words)
from google.colab import drive
drive.mount('/content/drive')

"""Installation"""

pip install surprise

!pip install anvil-uplink
import anvil.server
anvil.server.connect("DNOXCO5CLUGFCVMED5SR6P5H-QOSXMM7EFF76S4TW")

"""Function for Processing Raw Data"""

def csvDataProcess(name):
    data=pd.read_csv("/content/drive/MyDrive/"+name,encoding= 'unicode_escape')
    questions=[]
    length=len(data["Questions"])
    #add raw data into the relevant lists
    for i in range(length):
      questions.append(data["Questions"][i])
    #remove punctuation 
    dicts={i:'' for i in punctuation}
    punc_table=str.maketrans(dicts)
    for i in range(length):
      questions[i]=questions[i].translate(punc_table)
    #split sentences into word and remove stop words then reduced to a sentence
    pro_questions=[]
    for i in range(length):
      modified_sents=[]
      words = nltk.word_tokenize(questions[i])
      for word in words:
        if word not in stop_words:
          modified_sents.append(word)  
      pro_questions.append(' '.join(modified_sents))
    return pro_questions,questions

"""Import Data Set"""

questions1,ori_question1=csvDataProcess("Questions_1_25.csv")
questions2,ori_question2=csvDataProcess("Questions_26_50.csv")
questions3,ori_question3=csvDataProcess("Questions_51_75.csv")
questions4,ori_question4=csvDataProcess("Questions_76_98.csv")
pro_questions=[]
all_questions=[]
pro_questions.extend(questions1)
pro_questions.extend(questions2)
pro_questions.extend(questions3)
pro_questions.extend(questions4)

all_questions.extend(ori_question1)
all_questions.extend(ori_question2)
all_questions.extend(ori_question3)
all_questions.extend(ori_question4)
print(len(pro_questions))

"""Import Answers"""

data=pd.read_csv("/content/drive/MyDrive/"+"Mental_Health_FAQ.csv",encoding= 'unicode_escape')
data["Answers"]
answers=[]
for i in range(len(data["Answers"])):
  answers.append(data["Answers"][i])
print(len(answers))

answers_clean = []
def clean_text(text):
    # keep English, digital and space
    comp = re.compile(r'[A-Za-z0-9,.:;!_+-@$#&()?*=~`[]<>/"]')
    return comp.sub('', text)
for i in range(len(answers)):
  clean_word = answers[i]
  answers_clean.append(clean_text(clean_word))
answers = answers_clean
print(answers)
print(len(answers))

"""Feature Extraction

BOW
"""

bow_vect = CountVectorizer()
bow_train_counts = bow_vect.fit_transform(pro_questions)
bow_weight = bow_train_counts.toarray()
feature_word = bow_vect.get_feature_names();
print(bow_weight.shape)

"""TF-IDF"""

tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(bow_train_counts)
tf_idf_weight=X_train_tfidf.toarray()
print(tf_idf_weight.shape)

"""LDA"""

lda = LatentDirichletAllocation(n_components=98, max_iter=50,learning_method='batch')
X_train_lda = lda.fit_transform(bow_train_counts)
lda_weight = np.array(lda.fit_transform(bow_train_counts))
print(lda_weight)

"""Build Labels """

question_label=np.array([])
for i in range(98):
  temp=np.full((5,1),i,dtype=int)
  question_label=np.append(question_label,temp)
question_label = question_label.astype(int)
#print(question_label)

"""BOW+SVM"""

#prepare data
clf = svm.SVC(C=0.5, kernel='linear', decision_function_shape='ovo')
x_train,x_test,y_train,y_test=model_selection.train_test_split(bow_weight,question_label,random_state=1,train_size=0.8)
#train machine
clf.fit(x_train, y_train)
#accurcy
SVM_predicted = clf.predict(x_test)
print(np.mean(SVM_predicted == y_test.ravel()))
print(metrics.classification_report(y_test, SVM_predicted),'\n')

"""Function for Predicting using BOW+SVM"""

def bow_svm(str):
  Predict_part=docs_new = [str]
  Predict_counts = bow_vect.transform(Predict_part)
  Predict_input = Predict_counts.toarray()
  Predict_result = clf.predict(Predict_input)
  return answers[Predict_result[0]]
bow_svm("What is mental health?")

"""Confusion Matrix of BOW+SVM"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
#SVM Confusion Matrix
sns.set()
f2,ax3=plt.subplots(figsize=(30, 30))
SVM_confusion_matrix = confusion_matrix(y_test.ravel(),SVM_predicted,labels=range(98))
print('SVM confusion_matrix:')
print(SVM_confusion_matrix,'\n')
sns.heatmap(SVM_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('SVM confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""TF-IDF+SVM"""

x_train, x_test, y_train, y_test = model_selection.train_test_split(tf_idf_weight,question_label,random_state=1,train_size=0.8)
clf = svm.SVC(kernel='linear', C=1)
#train machine
clf.fit(x_train, y_train)
#accurcy
SVM_predicted = clf.predict(x_test)
print(np.mean(SVM_predicted == y_test.ravel()))
print(metrics.classification_report(y_test, SVM_predicted),'\n')

"""Function for Prediction using TF-IDF+SVM"""

def svm_prediction(question):
  Predict_part=docs_new = [question]
  X_new_counts = bow_vect.transform(Predict_part)
  Predict_counts = tfidf_transformer.transform(X_new_counts)
  Predict_input = Predict_counts.toarray()
  Predict_result = clf.predict(Predict_input)
  return answers[Predict_result[0]]
svm_prediction("What is mental health?")

"""Confusion Matrix of TF-IDF+SVM"""

sns.set()
f2,ax3=plt.subplots(figsize=(30, 30))
SVM_confusion_matrix = confusion_matrix(y_test.ravel(),SVM_predicted,labels=range(98))
print('SVM confusion_matrix:')
print(SVM_confusion_matrix,'\n')
sns.heatmap(SVM_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('SVM confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""BOW+KNN"""

#prepare
KNN = KNeighborsClassifier(n_neighbors=1,algorithm='kd_tree',weights = 'distance')
x_train, x_test, y_train, y_test = model_selection.train_test_split(bow_weight,question_label,random_state=1,train_size=0.75)
#train
KNN.fit(x_train, y_train)
#accurcy
KNN_predicted = KNN.predict(x_test)
print(np.mean(KNN_predicted == y_test.ravel()))
print(metrics.classification_report(y_test, KNN_predicted),'\n')

"""Prediction Function of BOW+KNN"""

def bow_knn(str):
  Predict_part=docs_new = [str]
  Predict_counts = bow_vect.transform(Predict_part)
  Predict_input = Predict_counts.toarray()
  Predict_result = KNN.predict(Predict_input)
  return(answers[Predict_result[0]])
bow_knn('What is mental health?')

"""Confusion Matrix of BOW+KNN"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
#KNN Confusion Matrix
sns.set()
f2,ax3=plt.subplots(figsize=(30, 30))
KNN_confusion_matrix = confusion_matrix(y_test.ravel(),KNN_predicted,labels=range(98))
print('KNN confusion_matrix:')
print(KNN_confusion_matrix,'\n')
sns.heatmap(KNN_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('KNN confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""TF_IDF+KNN"""

#prepare
KNN = KNeighborsClassifier(n_neighbors=2,algorithm='auto',weights = 'distance', p = 3)
x_train, x_test, y_train, y_test = model_selection.train_test_split(tf_idf_weight,question_label,random_state=1,train_size=0.75)
#train
KNN.fit(x_train, y_train)
#accurcy
KNN_predicted = KNN.predict(x_test)
print(np.mean(KNN_predicted == y_test.ravel()))
print(metrics.classification_report(y_test, KNN_predicted),'\n')

"""Prediction Function of TF_IDF+KNN"""

def tf_idf_knn(str):
  Predict_part=docs_new = [str]
  Predict_counts = bow_vect.transform(Predict_part)
  Predict_input = Predict_counts.toarray()
  Predict_result = KNN.predict(Predict_input)
  return answers[Predict_result[0]]
tf_idf_knn('what is mental health?')

"""Confusion Matrix of TF-IDF+KNN"""

sns.set()
f2,ax3=plt.subplots(figsize=(30, 30))
KNN_confusion_matrix = confusion_matrix(y_test.ravel(),KNN_predicted,labels=range(98))
print('KNN confusion_matrix:')
print(KNN_confusion_matrix,'\n')
sns.heatmap(KNN_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('KNN confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""BOW+Decision Tree"""

from sklearn import tree
#prepare
x_train, x_test, y_train, y_test = model_selection.train_test_split(bow_weight,question_label,random_state=1,train_size=0.8)
Tree = tree.DecisionTreeClassifier(splitter='best', criterion = 'gini')
#train
Tree.fit(x_train, y_train)
#accurcy
Tree_predicted = Tree.predict(x_test)
print(np.mean(Tree_predicted == y_test.ravel()))
print(metrics.classification_report(y_test, Tree_predicted),'\n')

"""Prediction Function of BOW+DC"""

def bow_dc(str):
  Predict_part=docs_new = [str]
  Predict_counts = bow_vect.transform(Predict_part)
  Predict_input = Predict_counts.toarray()
  Predict_result = Tree.predict(Predict_input)
  #return Predict_result,answers[Predict_result[0]]
  return answers[Predict_result[0]]
bow_dc('What is mental health?')

"""Confusion Matrix of BOW+DC"""

#DC Confusion Matrix
sns.set()
f2,ax3=plt.subplots(figsize=(30,30))
DC_confusion_matrix = confusion_matrix(y_test.ravel(),Tree_predicted,labels=range(98))
print('DC confusion_matrix:')
print(DC_confusion_matrix,'\n')
sns.heatmap(DC_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('DC confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""TF-IDF+Decision Tree"""

from sklearn import tree
#prepare
x_train, x_test, y_train, y_test = model_selection.train_test_split(tf_idf_weight,question_label,random_state=1,train_size=0.8)
Tree = tree.DecisionTreeClassifier(splitter='best')
#train
Tree.fit(x_train, y_train)
#accurcy
Tree_predicted = Tree.predict(x_test)
print(np.mean(Tree_predicted == y_test.ravel()))
print(metrics.classification_report(y_test, Tree_predicted),'\n')

"""Prediciton Function of TF_IDF+DC"""

def tf_idf_dc(str):
  Predict_part= docs_new = [str]
  Predict_counts = bow_vect.transform(Predict_part)
  Predict_counts= tfidf_transformer.transform(Predict_counts)
  Predict_input = Predict_counts.toarray()
  Predict_result = Tree.predict(Predict_input)
  return answers[Predict_result[0]]
tf_idf_dc('What is mental health?')

"""Confusion Matrix of TF-IDF+DC"""

sns.set()
f2,ax3=plt.subplots(figsize=(30, 30))
DC_confusion_matrix = confusion_matrix(y_test.ravel(),Tree_predicted,labels=range(98))
print('Decision Tree confusion_matrix:')
print(DC_confusion_matrix,'\n')
sns.heatmap(DC_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('Decision Tree confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""5 times Cross Vaildation"""

def fiveTimesCrossVaildation(machine,weight,feature,question_label=question_label):
    #svm
    scores= cross_val_score(machine, weight,question_label,cv=5) 
    #result of Cross-validation
    print("scores of 5 times cross validation using"+" "+feature)
    print("Cross-validation scores: {}".format(scores))
    #mean value
    print('mean of Cross-validation:')
    print(scores.mean())
fiveTimesCrossVaildation(Tree,bow_weight,"BOW+DC")
fiveTimesCrossVaildation(KNN,bow_weight,"BOW+KNN")
fiveTimesCrossVaildation(clf,bow_weight,"BOW+SVM")
fiveTimesCrossVaildation(Tree,tf_idf_weight,"TF_IDF+DC")
fiveTimesCrossVaildation(KNN,tf_idf_weight,"TF_IDF+KNN")
fiveTimesCrossVaildation(clf,tf_idf_weight,"TF_IDF+SVM")

"""KMEANS+EM+HC Core Code"""

# Commented out IPython magic to ensure Python compatibility.
def cluster_plot(weight,dim_value):
#   %matplotlib inline
  X = pd.DataFrame(weight)
  two_dim = PCA(n_components=2).fit_transform(X)
  n_dim = PCA(n_components=dim_value).fit_transform(X)
  X1 = pd.DataFrame(two_dim)
  X2 = pd.DataFrame(n_dim)
  #X2 = pd.DataFrame(X)
  y = pd.DataFrame(question_label)
  y.columns = ['Targets']
  # Build the KMeans Model
  kmeans = KMeans(n_clusters = 5, init='k-means++', n_init=10, max_iter=400, 
                  tol=0.0001, precompute_distances=True, verbose=0, random_state=9,
                  copy_x=True, n_jobs=None, algorithm='elkan')
  clusters1 = kmeans.fit_predict(X2)
  labels1 = np.zeros_like(clusters1)
  for i in range(5):
      cat = (clusters1 == i)
      labels1[cat] = mode(question_label[cat])[0]
  gmm = GaussianMixture(n_components=5, covariance_type='full', tol=0.0001, reg_covar=1e-06, 
                        max_iter=300, n_init=200,weights_init=None, means_init=None, 
                        precisions_init=None, random_state=None, warm_start=False,
                        verbose=0, verbose_interval=10)
  clusters2 = gmm.fit_predict(X2)
  labels2 = np.zeros_like(clusters2)
  for i in range(5):
      cat = (clusters2 == i)
      labels2[cat] = mode(question_label[cat])[0]
  from sklearn import preprocessing
  from sklearn.cluster import AgglomerativeClustering
  ac = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
  clusters3 = ac.fit_predict(X2)
  #print(hc)
  labels3 = np.zeros_like(clusters3)
  for i in range(5):
      cat = (clusters3 == i)
      labels3[cat] = mode(question_label[cat])[0] 
  X1.columns = ['xx', 'yy']
  plt.figure(figsize=(10, 10))
  colormap = np.array(['red', 'black', 'blue', 'green', 'yellow', 'orange', 'purple'])
  return labels1,labels2,labels3,X,X1,X2,clusters1,clusters2,clusters3

labels1,labels2,labels3,X,X1,X2,clusters1,clusters2,clusters3 = cluster_plot(tf_idf_weight,438)

"""Evaluation for KM EM and HC"""

def evaluation(labels_true,labels_pred,cluster,train_data):
  #kappa
  kappa= metrics.cohen_kappa_score(labels_true,labels_pred)
  #consistency
  consistency=metrics.adjusted_mutual_info_score(labels_true, labels_pred) 
  #silhouette
  silScore=metrics.silhouette_score(train_data,cluster)
  #coherence
  coherence=metrics.davies_bouldin_score(train_data,cluster)
  result=[kappa,consistency,silScore,coherence]
  return result
data=[evaluation(question_label,labels1,clusters1,X),
      evaluation(question_label,labels2,clusters2,X),
      evaluation(question_label,labels3,clusters3,X)]
form=pd.DataFrame(data,index=['KM ','EM ','HC '], columns=['kappa ','consistency ','silScore ','chScore '])
print(form)

def cluster_data(weight,dim_value):
 X = pd.DataFrame(tf_idf_weight)
 n_dim = PCA(n_components=dim_value).fit_transform(X)
 two_dim = PCA(n_components=2).fit_transform(X)
 n_dim = PCA(n_components=dim_value).fit_transform(X)
 X1 = pd.DataFrame(two_dim)
 X2 = pd.DataFrame(n_dim)
 y = pd.DataFrame(question_label)
 y.columns = ['Targets']
 return X,X1,X2,y

X,X1,X2,y = cluster_data(tf_idf_weight,50)
kmeans = KMeans(n_clusters = 5, init='k-means++', n_init=1, max_iter=200, 
                  tol=0.0001, precompute_distances=True, verbose=0, random_state=9,
                  copy_x=True, n_jobs=None, algorithm='auto')
clusters1 = kmeans.fit_predict(X2)
labels1 = np.zeros_like(clusters1)
for i in range(5):
   cat = (clusters1 == i)
   labels1[cat] = mode(question_label[cat])[0]
data= evaluation(question_label,labels1,clusters1,X)
#'kappa ','consistency ','silScore ','chScore '])
print('Kmeans')
form=pd.DataFrame(data,index=['kappa ','consistency ','silScore ','chScore '])
print(form)

X,X1,X2,y = cluster_data(tf_idf_weight,50)
gmm = GaussianMixture(n_components=5, covariance_type='full', tol=0.0001, reg_covar=1e-06, 
                        max_iter=300, n_init=200,weights_init=None, means_init=None, 
                        precisions_init=None, random_state=None, warm_start=False,
                        verbose=0, verbose_interval=10)
clusters2 = gmm.fit_predict(X2)
labels2 = np.zeros_like(clusters2)
for i in range(5):
  cat = (clusters2 == i)
  labels2[cat] = mode(question_label[cat])[0]
data= evaluation(question_label,labels2,clusters2,X)
#'kappa ','consistency ','silScore ','chScore '])
print('GaussianMixture')
form=pd.DataFrame(data,index=['kappa ','consistency ','silScore ','chScore '])
print(form)

from sklearn import preprocessing
from sklearn.cluster import AgglomerativeClustering
X,X1,X2,y = cluster_data(tf_idf_weight,50)
#"euclidean", "l1", "l2", "manhattan", "cosine", or "precomputed"
#linkage : {"ward", "complete", "average", "single"},
ac = AgglomerativeClustering(n_clusters=15, affinity='cosine', linkage='average')
clusters3 = ac.fit_predict(X)
  #print(hc)
labels3 = np.zeros_like(clusters3)
for i in range(5):
  cat = (clusters3 == i)
  labels3[cat] = mode(question_label[cat])[0] 
data= evaluation(question_label,labels3,clusters3,X)
#'kappa ','consistency ','silScore ','chScore '])
print('AgglomerativeClustering')
form=pd.DataFrame(data,index=['kappa ','consistency ','silScore ','chScore '])
print(form)

"""Error Analysis"""

import numpy as np
import matplotlib.pyplot as plt
def comp(elem):
  return elem[1]
#number_to_show: show the specific number of most common feature words in the bar chart
def error_analysis(weight,labels,book_label,feature_word,number_to_show):
  error_frequency_arr=[[i,0] for i in range(len(weight[0]))]
  for i in range(len(labels)):
    if labels[i]!=book_label[i]:
      for j in range(len(weight[i])):
        if weight[i][j]!=0:
          error_frequency_arr[j][1]+=weight[i][j]
    else:
      continue
  error_frequency_arr.sort(key=comp,reverse=True)
  error_label=[]
  error_frequency=[]
  pos=0
  for elem in error_frequency_arr:
    if elem[1]!=0 and pos<number_to_show:
      error_label.append(feature_word[elem[0]])
      error_frequency.append(elem[1])
      pos+=1
      continue
    else:
      break
  print(error_frequency)
  plt.barh(error_label[::-1],error_frequency[::-1])
  plt.show()
  return error_label

error_analysis(tf_idf_weight,labels1,question_label,feature_word,10)#KMeans Error Analysis
error_analysis(tf_idf_weight,labels2,question_label,feature_word,10)#EM Error Analysis
error_analysis(tf_idf_weight,labels3,question_label,feature_word,10)#HC Error Analysis

"""Redefine Stop Words List"""

#The execution of this function can only proceed after the error analysis, which belongs to the optimization stage
def redefine(weight,labels,question_label=question_label,feature_word=feature_word,orginial_stop_words=stop_words):
  customer_stop_words=set(error_analysis(weight,labels,question_label,feature_word,10))
  stop_words=orginial_stop_words | customer_stop_words
  return stop_words
stop_words=redefine(tf_idf_weight,labels1)
print(stop_words)

"""Clustering Confusion Matrix"""

sns.set()
f1,ax1=plt.subplots(figsize=(30, 30))
# Kmeans Confustion Matrix
KM_confusion_matrix = confusion_matrix(question_label,labels1,labels=range(98))
print('\n','KM confusion_matrix:')
print(KM_confusion_matrix,'\n')
sns.heatmap(KM_confusion_matrix,annot=True,ax = ax1) #Draw Heat map
ax1.set_title('KMeans confusion_matrix') #Label
ax1.set_xlabel('Predict Question') #X axis
ax1.set_ylabel('True Question') #Y axis
# EM Confusion Matrix
sns.set()
f2,ax2=plt.subplots(figsize=(30, 30))
EM_confusion_matrix = confusion_matrix(question_label,labels2,labels=range(98))
print('EM confusion_matrix:')
print(EM_confusion_matrix,'\n')
sns.heatmap(EM_confusion_matrix,annot=True,ax=ax2) #Draw Heat map
ax2.set_title('EM confusion_matrix') #Label
ax2.set_xlabel('Predict Question') #X axis
ax2.set_ylabel('True Question') #Y axis
# HC Confusion Matrix
sns.set()
f2,ax3=plt.subplots(figsize=(30, 30))
HC_confusion_matrix = confusion_matrix(question_label,labels3,labels=range(98))
print('HC confusion_matrix:')
print(HC_confusion_matrix,'\n')
sns.heatmap(HC_confusion_matrix,annot=True,ax=ax3) #Draw Heat map
ax3.set_title('Hierarchical clustering confusion_matrix') #Label
ax3.set_xlabel('Predict Question') #X axis
ax3.set_ylabel('True Question') #Y axis

"""Import Matrix as csv"""

import csv
def create_csv(matrix,csv_name):
    path = "/content/drive/MyDrive/"+csv_name+".csv"
    with open(path,'a+') as f:
        csv_write = csv.writer(f)
        csv_head = ["trueQuestion","predictQuestion","similarity"]
        csv_write.writerow(csv_head)

        rows,cols=matrix.shape
        for i in range(1,rows+1):
          for j in range(1,cols+1):
            csv_data_row=[i,j,matrix[i-1,j-1]+1]#推荐引擎中相似度最小为1，不可以存在0
            csv_write.writerow(csv_data_row)
    f.close()

create_csv(SVM_confusion_matrix,"SVM_confusion")
create_csv(KNN_confusion_matrix,"KNN_confusion")
create_csv(DC_confusion_matrix,"DC_confusion")

from google.colab import drive
drive.mount('/content/drive')

"""Recommend Engine"""

from datetime import datetime
import os
import pandas as pd
import numpy as np
from surprise import Reader
from surprise import Dataset
from surprise.model_selection import KFold
from surprise.model_selection import cross_validate
from surprise import NormalPredictor
from surprise import KNNBasic
from surprise import SVD
from surprise import NMF
from surprise import SlopeOne
from surprise import CoClustering
from surprise.accuracy import rmse
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import GridSearchCV
from collections import defaultdict

confusion = pd.read_csv('/content/drive/MyDrive/SVM_confusion.csv')
confusion.head()

simi_dict = {'trueQuestion': list(confusion.trueQuestion),
                'predictQuestion': list(confusion.predictQuestion),
                'similarity': list(confusion.similarity)}

df = pd.DataFrame(simi_dict)
df.shape

reader = Reader(rating_scale=(1.0, 6.0))
data = Dataset.load_from_df(df[['trueQuestion', 'predictQuestion', 'similarity']], reader)

data = Dataset.load_from_df(df[['trueQuestion', 'predictQuestion', 'similarity']], reader)
benchmark = []
for algorithm in [SVD(), NMF(), NormalPredictor(), KNNBasic()]:
    
    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)
    tmp = pd.DataFrame.from_dict(results).mean(axis=0)
    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]],index=['Algorithm']))
    benchmark.append(tmp)

surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')
surprise_results

trainset, testset = train_test_split(data, test_size=0.25)
algo = SVD()
predictions = algo.fit(trainset).test(testset)
accuracy.rmse(predictions)

trainset = data.build_full_trainset()
algo = SVD()
algo.fit(trainset)
testset = trainset.build_testset()
predictions = algo.test(testset)

def get_all_predictions(predictions):
    # First map the predictions to each user.
    top_n = defaultdict(list)    
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))
        
    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)

    return top_n

all_pred = get_all_predictions(predictions)

#To get top 5 reommendation
n = 5

for uid, user_ratings in all_pred.items():
    user_ratings.sort(key=lambda x: x[1], reverse=True)
    all_pred[uid] = user_ratings[:n]

tmp = pd.DataFrame.from_dict(all_pred)
tmp_transpose = tmp.transpose()

def get_predictions(trueQuestion):
    results = tmp_transpose.loc[trueQuestion]
    recommended_question_ids=[]
    for x in range(0, n):
      recommended_question_ids.append(results[x][0])
      recommended_questions=[]
    for question_id in recommended_question_ids:
      recommended_questions.append(all_questions[question_id])
    recommended_questions
    return recommended_questions

def svm_prediction(question):
  Predict_part=docs_new = [question]
  X_new_counts = bow_vect.transform(Predict_part)
  Predict_counts = tfidf_transformer.transform(X_new_counts)
  Predict_input = Predict_counts.toarray()
  Predict_result = clf.predict(Predict_input)
  return Predict_result[0]+1,answers[Predict_result[0]]

a,b = svm_prediction('what is mental illness')
get_predictions(a)

"""SVM Call function"""

@anvil.server.callable
def svm_prediction(question):
  Predict_part=docs_new = [question]
  X_new_counts = bow_vect.transform(Predict_part)
  Predict_counts = tfidf_transformer.transform(X_new_counts)
  Predict_input = Predict_counts.toarray()
  Predict_result = clf.predict(Predict_input)
  return Predict_result[0]+1,answers[Predict_result[0]]

@anvil.server.callable
def get_predictions(trueQuestion):
    results = tmp_transpose.loc[trueQuestion]
    recommended_question_ids=[]
    for x in range(0, n):
      recommended_question_ids.append(results[x][0])
      recommended_questions=[]
    for question_id in recommended_question_ids:
      recommended_questions.append(all_questions[question_id])
    recommended_questions
    return recommended_questions

anvil.server.wait_forever()